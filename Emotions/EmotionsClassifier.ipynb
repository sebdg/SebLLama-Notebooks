{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install  pandas transformers datasets matplotlib seaborn scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_parquet(\"go_emotions_train.parquet\")\n",
    "test = pd.read_parquet(\"go_emotions_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = train.columns[1:29].values.tolist()\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_emotions_to_string(array_of_labels):\n",
    "    \"\"\"Decode the emotions from the row to a list of strings.\"\"\"\n",
    "    # active labels are 1, inactive are 0\n",
    "    return [emotion for i, emotion in enumerate(emotions) if array_of_labels[i] == 1]\n",
    "\n",
    "\n",
    "def decode_emotions_to_index(row):\n",
    "    \"\"\"Decode the emotions from the row to a list of indices.\"\"\"\n",
    "    return [i for i, emotion in enumerate(emotions) if row[emotion] == 1]\n",
    "\n",
    "def decode_logits(row, k=3, alphabetic_sort=False):\n",
    "    \"\"\"Decode the logits from the row to a list of strings.\"\"\"\n",
    "    top = np.argsort(row)[-k:]\n",
    "    if alphabetic_sort:\n",
    "        top.sort()\n",
    "    return [emotions[i] for i in top]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "train[\"length_text\"] = train[\"text\"].apply(len)\n",
    "train['length_text'].plot(kind='hist', title='Text length distribution')\n",
    "\n",
    "test[\"length_text\"] = test[\"text\"].apply(len)\n",
    "test['length_text'].plot(kind='hist', title='Text length distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with text length > max_sequence_length   \n",
    "train = train[train[\"length_text\"] <= max_sequence_length]\n",
    "test = test[test[\"length_text\"] <= max_sequence_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "train[\"length_text\"] = train[\"text\"].apply(len)\n",
    "train['length_text'].plot(kind='hist', title='Text length distribution')\n",
    "\n",
    "test[\"length_text\"] = test[\"text\"].apply(len)\n",
    "test['length_text'].plot(kind='hist', title='Text length distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_tokens = [ \n",
    "    # smileys\n",
    "    \":)\", \";)\", \":P\", \":D\", \":(\", \":'(\", \":O\", \":/\", \":|\", \":*\", \":@\", \">:(\", \n",
    "    # emojis and their unicode representation\n",
    "    \":thumbsup:\", \"ğŸ‘\", \":thumbsdown:\", \"ğŸ‘\", \":clap:\", \"ğŸ‘\", \":wave:\", \"ğŸ‘‹\", \":pray:\", \"ğŸ™\", \n",
    "    \":smile:\", \"ğŸ˜„\", \":grinning:\", \"ğŸ˜€\", \":laughing:\", \"ğŸ˜†\", \":sweat_smile:\", \"ğŸ˜…\", \":rofl:\", \"ğŸ¤£\", \n",
    "    \":blush:\", \"ğŸ˜Š\", \":innocent:\", \"ğŸ˜‡\", \":wink:\", \"ğŸ˜‰\", \":relieved:\", \"ğŸ˜Œ\", \":heart_eyes:\", \"ğŸ˜\", \n",
    "    \":kissing_heart:\", \"ğŸ˜˜\", \":kissing:\", \"ğŸ˜—\", \":kissing_smiling_eyes:\", \"ğŸ˜™\", \":kissing_closed_eyes:\", \"ğŸ˜š\", \n",
    "    \":yum:\", \"ğŸ˜‹\", \":stuck_out_tongue:\", \"ğŸ˜›\", \":stuck_out_tongue_winking_eye:\", \"ğŸ˜œ\", \n",
    "    \":stuck_out_tongue_closed_eyes:\", \"ğŸ˜\", \":money_mouth_face:\", \"ğŸ¤‘\", \":hugs:\", \"ğŸ¤—\", \":smirk:\", \"ğŸ˜\", \n",
    "    \":unamused:\", \"ğŸ˜’\", \":disappointed:\", \"ğŸ˜\", \":pensive:\", \"ğŸ˜”\", \":worried:\", \"ğŸ˜Ÿ\", \":confused:\", \"ğŸ˜•\", \n",
    "    \":persevere:\", \"ğŸ˜£\", \":confounded:\", \"ğŸ˜–\", \":tired_face:\", \"ğŸ˜«\", \":weary:\", \"ğŸ˜©\", \":cry:\", \"ğŸ˜¢\", \n",
    "    \":sob:\", \"ğŸ˜­\", \":frowning:\", \"â˜¹ï¸\", \":anguished:\", \"ğŸ˜§\", \":fearful:\", \"ğŸ˜¨\", \":cold_sweat:\", \"ğŸ˜°\", \n",
    "    \":disappointed_relieved:\", \"ğŸ˜¥\", \":sweat:\", \"ğŸ˜“\", \":hugging_face:\", \"ğŸ¤—\", \":thinking:\", \"ğŸ¤”\", \n",
    "    \":shushing_face:\", \"ğŸ¤«\", \":lying_face:\", \"ğŸ¤¥\", \":no_mouth:\", \"ğŸ˜¶\", \":neutral_face:\", \"ğŸ˜\", \n",
    "    \":expressionless:\", \"ğŸ˜‘\", \":grimacing:\", \"ğŸ˜¬\", \":rolling_eyes:\", \"ğŸ™„\", \":hushed:\", \"ğŸ˜¯\", \n",
    "    \":frowning2:\", \"â˜¹ï¸\", \":anguished:\", \"ğŸ˜§\", \":open_mouth:\", \"ğŸ˜®\", \":astonished:\", \"ğŸ˜²\", \n",
    "    \":sleeping:\", \"ğŸ˜´\", \":drooling_face:\", \"ğŸ¤¤\", \":sleepy:\", \"ğŸ˜ª\", \":dizzy_face:\", \"ğŸ˜µ\", \n",
    "    \":zipper_mouth_face:\", \"ğŸ¤\", \":nauseated_face:\", \"ğŸ¤¢\", \":sneezing_face:\", \"ğŸ¤§\", \":mask:\", \"ğŸ˜·\", \n",
    "    \":thermometer_face:\", \"ğŸ¤’\", \":head_bandage:\", \"ğŸ¤•\", \":smiling_imp:\", \"ğŸ˜ˆ\", \":imp:\", \"ğŸ‘¿\", \n",
    "    \":japanese_ogre:\", \"ğŸ‘¹\", \":japanese_goblin:\", \"ğŸ‘º\", \":skull:\", \"ğŸ’€\", \":ghost:\", \"ğŸ‘»\", \":alien:\", \"ğŸ‘½\", \n",
    "    \":robot:\", \"ğŸ¤–\", \":poop:\", \"ğŸ’©\", \":smiley_cat:\", \"ğŸ˜º\", \":smile_cat:\", \"ğŸ˜¸\", \":joy_cat:\", \"ğŸ˜¹\", \n",
    "    \":heart_eyes_cat:\", \"ğŸ˜»\", \":smirk_cat:\", \"ğŸ˜¼\", \":kissing_cat:\", \"ğŸ˜½\", \":scream_cat:\", \"ğŸ™€\", \n",
    "    \":crying_cat_face:\", \"ğŸ˜¿\", \":pouting_cat:\", \"ğŸ˜¾\", \":raised_hands:\", \"ğŸ™Œ\", \":clap:\", \"ğŸ‘\", \n",
    "    \":wave:\", \"ğŸ‘‹\", \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Tokenize the text data\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Add additional tokens to the tokenizer\n",
    "tokenizer.add_tokens(additional_tokens)\n",
    "\n",
    "max_sequence_length = 200\n",
    "\n",
    "train_sequences = tokenizer(train['text'].tolist(), \n",
    "    padding='max_length', truncation=True, \n",
    "    max_length=max_sequence_length, return_tensors='np', \n",
    "    return_token_type_ids=False, \n",
    "    return_attention_mask=False)\n",
    "train_labels = train[emotions].values\n",
    "print(len(train_sequences['input_ids']))\n",
    "\n",
    "test_sequences = tokenizer(test['text'].tolist(),\n",
    "    padding='max_length', truncation=True, \n",
    "    max_length=max_sequence_length, return_tensors='np', \n",
    "    return_token_type_ids=False, \n",
    "    return_attention_mask=False)\n",
    "test_labels = test[emotions].values\n",
    "print(len(test_sequences['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences[:5], test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_coarse = {\n",
    "    'admiration': 'positive',\n",
    "    'amusement': 'positive',\n",
    "    'anger': 'negative',\n",
    "    'annoyance': 'negative',\n",
    "    'approval': 'positive',\n",
    "    'caring': 'positive',\n",
    "    'confusion': 'negative',\n",
    "    'curiosity': 'positive',\n",
    "    'desire': 'positive',\n",
    "    'disappointment': 'negative',\n",
    "    'disapproval': 'negative',\n",
    "    'disgust': 'negative',\n",
    "    'embarrassment': 'negative',\n",
    "    'excitement': 'positive',\n",
    "    'fear': 'negative',\n",
    "    'gratitude': 'positive',\n",
    "    'grief': 'negative',\n",
    "    'joy': 'positive',\n",
    "    'love': 'positive',\n",
    "    'nervousness': 'negative',\n",
    "    'optimism': 'positive',\n",
    "    'pride': 'positive',\n",
    "    'realization': 'positive',\n",
    "    'relief': 'positive',\n",
    "    'remorse': 'negative',\n",
    "    'sadness': 'negative',\n",
    "    'surprise': 'positive',\n",
    "    'neutral': 'neutral'\n",
    "}\n",
    "\n",
    "# Create lists of emotions for each coarse category\n",
    "positive_emotions = [emotion for emotion, category in emotion_to_coarse.items() if category == 'positive']\n",
    "negative_emotions = [emotion for emotion, category in emotion_to_coarse.items() if category == 'negative']\n",
    "neutral_emotions =  [emotion for emotion, category in emotion_to_coarse.items() if category == 'neutral']\n",
    "\n",
    "# Sum the values for positive and negative emotions for each row\n",
    "train['positive_sum'] = train[positive_emotions].sum(axis=1)\n",
    "train['negative_sum'] = train[negative_emotions].sum(axis=1)\n",
    "train['neutral_sum'] = train[neutral_emotions].sum(axis=1)\n",
    "\n",
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_sequences['input_ids']\n",
    "y_train = train_labels.astype(np.float32)\n",
    "X_test = test_sequences['input_ids']\n",
    "y_test = test_labels.astype(np.float32)\n",
    "\n",
    "\n",
    "print(type(X_train[0]), type(y_train[0]))\n",
    "print(type(X_train[0][0]), type(y_train[0][0]))\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf ; print(\"Num GPUs Available: \", tf.config.experimental.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 28  # Adjust based on your dataset\n",
    "embedding_dim = 256  # Embedding dimension\n",
    "max_sequence_length = 200  # Example sequence length, adjust as needed\n",
    "tokenizer_vocab_size = tokenizer.vocab_size  # Example vocabulary size, adjust as needed\n",
    "tokenizer_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convultional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input, BatchNormalization, LSTM, Bidirectional, Reshape\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Number of classes for multi-label classification\n",
    "\n",
    "# Define the CNN model architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(max_sequence_length,)),\n",
    "    Embedding(input_dim=tokenizer_vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),  # Embedding layer\n",
    "    Conv1D(filters=128, kernel_size=32, activation='relu'),  # Conv1D layer\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(2048, activation='relu'),  # Dense layer\n",
    "    Dense(1512, activation='relu'),  # Dense layer\n",
    "    Dense(1024, activation='relu'),  # Dense layer\n",
    "    Conv1D(filters=128, kernel_size=16, activation='relu'),  # Conv1D layer\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(256, activation='relu'),  # Dense layer\n",
    "    Dense(128, activation='relu'),  # Dense layer\n",
    "    Conv1D(filters=96, kernel_size=8, activation='relu'),  # Conv1D layer\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(128, activation='relu'),  # Dense layer\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),  # Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=8, activation='relu'),  # Conv1D layer\n",
    "    GlobalMaxPooling1D(),  # Global max pooling\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(128, activation='relu'),  # Dense layer\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(64, activation='relu'),  # Dense layer\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(num_classes, activation='sigmoid')  # Output layer for multi-label classification\n",
    "])\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LayerNormalization, Embedding, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 256  # Embedding dimension\n",
    "max_sequence_length = 200  # Example sequence length, adjust as needed\n",
    "tokenizer_vocab_size = tokenizer.vocab_size  # Example vocabulary size, adjust as needed\n",
    "num_classes = 28  # Adjust based on your dataset\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    return x + res\n",
    "\n",
    "def build_transformer_model():\n",
    "    inputs = Input(shape=(200,))\n",
    "    embedding_layer = Embedding(input_dim=tokenizer_vocab_size, output_dim=embedding_dim, input_length=200)(inputs)\n",
    "    x = transformer_encoder(embedding_layer, head_size=256, num_heads=8, ff_dim=384, dropout=0.3)\n",
    "    x = transformer_encoder(x, head_size=256, num_heads=256, ff_dim=384, dropout=0.3)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_transformer_model()\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(X_train[0:1])\n",
    "decoded = decode_logits(result[0], k=3, alphabetic_sort=True)\n",
    "y_train[0]\n",
    "\n",
    "print(result)\n",
    "print(decoded)\n",
    "print(decode_emotions_to_string(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Dropout, concatenate, Flatten\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(input_dim=tokenizer_vocab_size, output_dim=max_sequence_length)(input_layer)\n",
    "dropout_layer = Dropout(rate=0.5)(embedding_layer)\n",
    "flat_layer = Flatten()(dropout_layer)\n",
    "\n",
    "# Coarse-grained classification layer\n",
    "coarse_output_layer = Dense(units=3, activation='softmax')(flat_layer)\n",
    "\n",
    "# Fine-grained classification layer\n",
    "fine_output_layer = Dense(units=128, activation='relu')( concatenate([coarse_output_layer, flat_layer]))\n",
    "fine_output_layer = Dropout(rate=0.5)(fine_output_layer)\n",
    "fine_output_layer = Dense(units=28, activation='sigmoid')(fine_output_layer)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=fine_output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train is your multi-label binary matrix (samples x labels)\n",
    "# Flatten y_train to a 1D array for each label\n",
    "class_weights = []\n",
    "for i in range(y_train.shape[1]):\n",
    "    class_weight = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=y_train[:, i])\n",
    "    class_weights.append(class_weight)\n",
    "\n",
    "# Convert to a dictionary format suitable for Keras\n",
    "class_weight_dict = {i: {0: class_weights[i][0], 1: class_weights[i][1]} for i in range(len(class_weights))}\n",
    "\n",
    "print(class_weight_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_weighted_binary_crossentropy(class_weight_dict):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Initialize the total loss\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Loop through each label and compute the weighted binary crossentropy\n",
    "        for i in range(y_true.shape[1]):\n",
    "            bce = tf.keras.losses.binary_crossentropy(y_true[:, i], y_pred[:, i])\n",
    "            weights = y_true[:, i] * class_weight_dict[i][1] + (1 - y_true[:, i]) * class_weight_dict[i][0]\n",
    "            weighted_bce = weights * bce\n",
    "            total_loss += tf.reduce_mean(weighted_bce)\n",
    "        \n",
    "        # Return the average loss over all labels\n",
    "        return total_loss / y_true.shape[1]\n",
    "    return loss\n",
    "\n",
    "# Compile the model with the custom multi-label loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr, warmup=3, steps=10):\n",
    "    if epoch < 3:\n",
    "        return lr * 1.3\n",
    "    elif epoch % steps == 0:\n",
    "        return lr * 0.9\n",
    "    else:\n",
    "        return lr * 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "lrs = []\n",
    "for epoch in range(20):\n",
    "    lr = scheduler(epoch, lr, 3, 5)\n",
    "    lrs.append(lr)\n",
    "    print(epoch, lr)\n",
    "\n",
    "# plot learning rate schedule\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lrs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.title('Learning rate schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt    \n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# add tensorboard callback\n",
    "import datetime\n",
    "log_dir = \"./logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=True, embeddings_freq=1)\n",
    "\n",
    "\n",
    "# Learning rate scheduler callback\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "initial_lr = 1e-4\n",
    "\n",
    "auc = keras.metrics.AUC(name='auc')\n",
    "model.compile(optimizer=Adam(learning_rate=initial_lr), loss=multi_label_weighted_binary_crossentropy(class_weight_dict), metrics=['accuracy', 'Precision', 'Recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, precision, recall  = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Test AUC: {auc_ths.tolist()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
